---
phase: 05-intake-and-parsing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/PARSING_PIPELINE_PLAN.md
autonomous: true

must_haves:
  truths:
    - "The parsing pipeline plan specifies Docling + Unstructured orchestration, canonical JSON schema with field definitions, lineage pointer structure, and parse failure reporting"
    - "Detail sufficient to write the canonical schema as a Pydantic model"
  artifacts:
    - path: "docs/PARSING_PIPELINE_PLAN.md"
      provides: "Step-by-step parsing pipeline implementation plan"
      min_lines: 450
---

<objective>
Create the parsing pipeline implementation plan: Docling + Unstructured orchestration, canonical structured document JSON schema with field definitions, lineage pointer structure, deterministic chunk ID generation, parse failure reporting. An engineer must be able to write the canonical schema as a Pydantic model and implement the pipeline without asking clarifying questions.

Output: `docs/PARSING_PIPELINE_PLAN.md`
</objective>

<context>
@docs/PRD.md (Section 2.2 Phase B: parsing stages, canonical JSON schema, error handling)
@docs/TECH_DECISIONS.md (Docling >=2.70, Unstructured >=0.16, Pydantic >=2.10)
@docs/AUDIT_ARCHITECTURE.md (Section 1.2: DOCUMENT_PARSED, DOCUMENT_PARSE_FAILED, CONTENT_DROPPED)
@docs/STORAGE_LAYER_PLAN.md (MinIO paths: normalized/{tenant_id}/{doc_id}/structured.json)
</context>

<tasks>

<task type="auto">
  <name>Write parsing pipeline implementation plan</name>
  <files>docs/PARSING_PIPELINE_PLAN.md</files>
  <action>
Create `docs/PARSING_PIPELINE_PLAN.md` with:

**1. Overview**
- Input: raw files from object store (intake receipt: file_id, sha256, mime_type)
- Output: canonical structured JSON at normalized/{tenant_id}/{doc_id}/structured.json
- Reference PRD Section 2.2

**2. Stage 1: Layout-Aware Conversion (Docling)**
- Docling >=2.70 per TECH_DECISIONS
- Extract: sections (section_id, title, level, page, start_char, end_char), tables (table_id, page, rows, columns, cells, offsets), figures (figure_id, caption, page, offsets), reading_order
- OCR fallback for scanned (PNG, TIFF, image PDF): confidence scoring, low-confidence flagging
- Supported formats: PDF, DOCX, XLSX, TXT, CSV, PNG, TIFF

**3. Stage 2: Partitioning and Chunking (Unstructured)**
- Unstructured >=0.16 per TECH_DECISIONS
- Strategy: by_title (respect section breaks, never split tables)
- Deterministic chunk_id: hash(doc_id + page + start_offset + end_offset)
- Element types: paragraph, table, heading, list_item, figure_caption
- Metadata per chunk: section_title, heading_level

**4. Stage 3: Canonical JSON Assembly**
- Merge layout (Stage 1) + chunks (Stage 2)
- Lineage: raw_sha256, stage1_parser_version, stage2_parser_version, parse_timestamp
- Stats: page_count, section_count, table_count, chunk_count, ocr_pages, ocr_avg_confidence
- Write to object store

**5. Canonical JSON Schema (Pydantic-Ready)**
- Complete field definitions: doc_id, file_id, tenant_id, sections[], tables[], figures[], reading_order[], chunks[], lineage, stats
- Chunk schema: chunk_id, text, page, start_char, end_char, element_type, metadata
- Enough detail to implement as Pydantic BaseModel

**6. Parse Failure Reporting**
- DOCUMENT_PARSE_FAILED: entire file unparseable (FILE_CORRUPTED, etc.)
- CONTENT_DROPPED: partial extraction, flag in stats
- Acceptance report integration

**7. Audit Events**
- DOCUMENT_PARSED, DOCUMENT_PARSE_FAILED, CONTENT_DROPPED per AUDIT_ARCHITECTURE Section 1.2

**8. Job Enqueue**
- After successful parse: enqueue policy check to tenant queue
  </action>
</task>

</tasks>

<success_criteria>
- docs/PARSING_PIPELINE_PLAN.md exists, >= 450 lines
- Docling + Unstructured orchestration with stage boundaries
- Canonical schema with Pydantic-ready field definitions
- Lineage and chunk_id generation specified
- Parse failure and audit event handling
</success_criteria>

<output>
Create .planning/phases/05-intake-and-parsing/05-02-SUMMARY.md
</output>
